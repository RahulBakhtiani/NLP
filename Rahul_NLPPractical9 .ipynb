{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Rahul_NLPPractical9 .ipynb","provenance":[{"file_id":"1Bit0VezAonmLhNtGSsYcFoBme5d6F-Oj","timestamp":1649506606677}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Practical No. 9\n","# Implement Naive Bayes classifier\n"],"metadata":{"id":"3g9EQyezIuXo"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"57v7oOa3IoQR","executionInfo":{"status":"ok","timestamp":1649506840613,"user_tz":-330,"elapsed":1317,"user":{"displayName":"RaHuL Bakhtiani","userId":"07799808178888971061"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","sms_data = pd.read_csv(\"/content/spam (2).csv\", encoding='latin-1')"]},{"cell_type":"code","source":["import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wy09-dyKpih","executionInfo":{"status":"ok","timestamp":1649506846984,"user_tz":-330,"elapsed":2887,"user":{"displayName":"RaHuL Bakhtiani","userId":"07799808178888971061"}},"outputId":"f944d888-22a3-464f-b8e6-19e6dcef1fd8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["stemming = PorterStemmer()\n","corpus = []\n","for i in range (0,len(sms_data)):\n","  s1 = re.sub('[^a-zA-Z]',repl = ' ',string = sms_data['v2'][i])\n","  s1.lower()\n","  s1 = s1.split()\n","  s1 = [stemming.stem(word) for word in s1 if word not in set(stopwords.words('english'))]\n","  s1 = ' '.join(s1)\n","  corpus.append(s1)\n","from sklearn.feature_extraction.text import CountVectorizer\n","countvectorizer =CountVectorizer()\n","x = countvectorizer.fit_transform(corpus).toarray()\n","print(x)\n","y = sms_data['v1'].values\n","print(y)\n","from sklearn.model_selection import train_test_split\n","x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, \n","stratify=y,random_state=2)\n","#Multinomial Na√Øve Bayes.\n","from sklearn.naive_bayes import MultinomialNB\n","multinomialnb = MultinomialNB()\n","multinomialnb.fit(x_train,y_train)\n","# Predicting on test data:\n","y_pred = multinomialnb.predict(x_test)\n","print(y_pred)\n","#Results of our Models\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import accuracy_score\n","print(classification_report(y_test,y_pred))\n","print(\"accuracy_score: \",accuracy_score(y_test,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZtDVDOHKsu0","executionInfo":{"status":"ok","timestamp":1649506865637,"user_tz":-330,"elapsed":15694,"user":{"displayName":"RaHuL Bakhtiani","userId":"07799808178888971061"}},"outputId":"5e99d97b-ecb0-4a30-f938-14b34c05767e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n","['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n","['ham' 'ham' 'ham' ... 'ham' 'ham' 'ham']\n","              precision    recall  f1-score   support\n","\n","         ham       0.99      0.99      0.99      1448\n","        spam       0.92      0.93      0.92       224\n","\n","    accuracy                           0.98      1672\n","   macro avg       0.95      0.96      0.96      1672\n","weighted avg       0.98      0.98      0.98      1672\n","\n","accuracy_score:  0.979066985645933\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"k7yOslbdLJK-"},"execution_count":null,"outputs":[]}]}